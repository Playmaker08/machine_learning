## In this exercise, we will see one of the ways that this decomposition can be useful. To do this, we will construct a dataset that represents grade scores for 100 students in 24 different subjects. The overall average has been removed so this data represents the percentage point each student received above or below the average test score. So a 0 represents an average grade (C), a 25 is a high grade (A+), and a -25 represents a low grade (F). You can simulate the data like this:

set.seed(1987)
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))
                 
## Our goal is to describe the student performances as succinctly as possible. For example, we want to know if these test results are all just a random independent numbers. Are all students just about as good? Does being good in one subject  imply you will be good in another? How does the SVD help with all this? We will go step by step to show that with just three relatively small pairs of vectors we can explain much of the variability in this 100 x 24 dataset. 


## visualize the 24 test scores for the 100 students by plotting an image
  my_image <- function(x, zlim = range(x), ...){
	colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
	cols <- 1:ncol(x)
	rows <- 1:nrow(x)
	image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
			xlab="", ylab="",  col = colors, zlim = zlim, ...)
	abline(h=rows + 0.5, v = cols + 0.5)
	axis(side = 1, cols, colnames(x), las = 2)
  }
  my_image(y)


## examine the correlation between the test scores directly like this
my_image(cor(y), zlim = c(-1,1))
range(cor(y))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)


## perform the Singular Value Decomposition (SVD) on y, check its validity, and calculate the sum of squares as required
# Perform Singular Value Decomposition (SVD)
s <- svd(y)
# Check the names of the components of the SVD
names(s)
# Reconstruct y using the SVD components
y_svd <- s$u %*% diag(s$d) %*% t(s$v)
# Confirm the reconstruction by comparing y and y_svd
max_diff <- max(abs(y - y_svd))  # Should be very close to 0
max_diff
# Compute the sum of squares of the columns of y
ss_y <- sum(y^2)
# Compute the transformed matrix YV and its sum of squares
y_v <- y %*% s$v
ss_yv <- sum(y_v^2)
# Confirm that the total variability is preserved
list(
  sum_ss_y = ss_y,
  sum_ss_yv = ss_yv,
  check = all.equal(ss_y, ss_yv)  # Should return TRUE
)
ss_y <- apply(y^2, 2, sum)
  ss_yv <- apply((y%*%s$v)^2, 2, sum)
  sum(ss_y)
  sum(ss_yv)


## We see that the total sum of squares is preserved. This is because V is orthogonal. Now to start understanding how YV is useful, plot ss_y against the column number and then do the same for ss_yv
# Compute the sum of squares for each column of y
ss_y_columns <- colSums(y^2)

# Compute the sum of squares for each column of the transformed matrix YV
y_v <- y %*% s$v
ss_yv_columns <- colSums(y_v^2)

# Plot ss_y_columns against column numbers
plot(ss_y_columns, type = "b", pch = 19, col = "blue",
     xlab = "Column Number", ylab = "Sum of Squares",
     main = "Sum of Squares for y (ss_y)")

# Plot ss_yv_columns against column numbers
plot(ss_yv_columns, type = "b", pch = 19, col = "red",
     xlab = "Column Number", ylab = "Sum of Squares",
     main = "Sum of Squares for YV (ss_yv)")


## calculate the proportion of total variability explained by the first three columns of YV
# Compute the squared singular values (s$d^2)
singular_values_squared <- s$d^2

# Compute the total variability (sum of squared singular values)
total_variability <- sum(singular_values_squared)

# Compute the proportion of variability explained by the first three columns
proportion_explained <- sum(singular_values_squared[1:3]) / total_variability
proportion_explained


## to compute and visualize the relationship between the average score for each student and U1*d1,1
# Compute the first column of UD (U_1 * d_1,1)
u1_d1 <- s$u[, 1] * s$d[1]

# Compute the average score for each student
average_scores <- rowMeans(y)

# Plot the relationship
plot(
  u1_d1, average_scores,
  xlab = "U1 * d1,1",
  ylab = "Average Score",
  main = "Relationship between Average Score and U1 * d1,1",
  pch = 19, col = "blue"
)
abline(lm(average_scores ~ u1_d1), col = "red", lwd = 2)  # Add linear regression line
